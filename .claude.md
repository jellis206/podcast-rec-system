# Claude Project Context: Podcast Recommendation System

## Project Overview

This is a vector database project that builds a podcast recommendation system using PostgreSQL with the pgvector extension. The system uses 128-dimensional vector embeddings from OpenAI to find similar podcast segments and episodes from the Lex Fridman podcast dataset.

## Key Technical Details

### Technology Stack

- **Database**: PostgreSQL with pgvector extension (TimescaleDB)
- **Language**: Python 3.8+
- **Key Libraries**: psycopg2, pgcopy, datasets, pandas, python-dotenv
- **Environment**: python -m venv (NOT conda - keep things simple and local)
- **Output Format**: Markdown (NOT PDF - user will convert later if needed)

### Dataset Information

- **Source**: Lex Fridman Podcast from HuggingFace
- **Scale**: 346 podcasts, 832,839 segments
- **Embeddings**: 128-dimensional vectors from OpenAI
- **Segment Definition**: 1-2 sentences uttered by same speaker

### Data Sources

1. **batch_request_*.jsonl** - Raw podcast transcripts and metadata
2. **embeddings.jsonl** - 128-dimensional vectors (custom_id field matches segment IDs)
3. **HuggingFace dataset** - Additional metadata via `load_dataset("Whispering-GPT/lex-fridman-podcast")`

## Database Schema

### podcast table

```sql
CREATE TABLE podcast (
    id TEXT PRIMARY KEY,          -- e.g., "TRdL6ZzWBS0"
    title TEXT                     -- e.g., "Jed Buchwald: Isaac Newton... | Lex Fridman Podcast #214"
);
```

### segment table

```sql
CREATE TABLE segment (
    id TEXT PRIMARY KEY,           -- Format: "podcast_idx;segment_idx" (e.g., "0;1")
    start_time FLOAT,              -- Segment start timestamp
    end_time FLOAT,                -- Segment end timestamp
    content TEXT,                  -- Raw transcript text
    embedding VECTOR(128),         -- 128-dimensional vector from OpenAI
    podcast_id TEXT,               -- Foreign key to podcast.id
    FOREIGN KEY (podcast_id) REFERENCES podcast(id)
);
```

### Important Schema Notes

- Segment ID format: "0;1" means podcast index 0, segment index 1
- This matches the "custom_id" field in embeddings.jsonl and batch_request.jsonl
- The pgvector extension must be enabled: `CREATE EXTENSION vector`

## Project Structure

### Files to Implement

1. **db_build.py** - Create tables
   - Enable pgvector extension (code provided)
   - Write CREATE TABLE statements for podcast and segment
   - Execute table creation using psycopg2

2. **db_insert.py** - Populate database
   - Read batch_request_*.jsonl files
   - Read embeddings.jsonl file
   - Load HuggingFace dataset
   - Create pandas DataFrames
   - Use `fast_pg_insert()` from utils.py for efficient bulk insertion
   - Insert ~346 podcasts
   - Insert ~832,839 segments

3. **db_query.py** - Query similar segments/episodes
   - Use L2 distance operator: `<->`
   - Exclude query document from results
   - Answer 6 specific questions (see below)

### Utility Files

- **utils.py** - Contains `fast_pg_insert()` for efficient bulk insertion using psycopg2's COPY
- **db_drop.py** - Utility to drop tables
- **.env** - Stores DATABASE_URL connection string (not tracked in git)

## Query Requirements

All queries use **L2 distance** (`<->` operator) and exclude the query document from results.

### Q1: Most Similar to Segment "267:476"
- Input: "that if we were to meet alien life at some point"
- Return: podcast name, segment id, text, start_time, end_time, distance
- Limit: 5 results

### Q2: Most Dissimilar to Segment "267:476"
- Same input as Q1
- Return: same fields
- Find furthest embeddings (largest L2 distance)

### Q3: Most Similar to Segment "48:511"
- Input: "Is it is there something especially interesting and profound to you in terms of our current deep learning neural network..."
- Return: same fields as Q1

### Q4: Most Similar to Segment "51:56"
- Input: "But what about like the fundamental physics of dark energy? Is there any understanding of what the heck it is?"
- Return: same fields as Q1

### Q5: Most Similar Episodes (by segment)
- For segments: "267:476", "48:511", "51:56"
- Find 5 most similar **episodes** (not segments)
- Method: Average embeddings within each podcast episode
- Return: podcast title, distance

### Q6: Most Similar Episodes (by episode)
- Input episode: "VeH7qKZr0WI" (Balaji Srinivasan episode)
- Find 5 most similar episodes
- Method: Average embeddings as in Q5
- Return: podcast title, distance

## Vector Operations in pgvector

```sql
-- L2 distance (Euclidean) - USE THIS
SELECT * FROM segment ORDER BY embedding <-> '[...]' LIMIT 5;

-- Other available operators (not used in this project):
-- <#> - (negative) inner product
-- <=> - cosine distance
-- <+> - L1 distance
```

## Implementation Notes

### Data Insertion Performance

- CRITICAL: Use `fast_pg_insert()` from utils.py
- Do NOT use individual INSERT statements for 800k+ records
- The fast_pg_insert uses psycopg2's COPY command via pgcopy
- Expected time: minutes (not hours)

### Connection Management

```python
from dotenv import load_dotenv
import os

load_dotenv()
CONNECTION = os.getenv('DATABASE_URL')
```

### Data Parsing Tips

- batch_request_*.jsonl: One JSON object per line, not a JSON array
- embeddings.jsonl: custom_id field maps to segment.id
- Need to correlate data across multiple sources
- Pandas DataFrames are recommended for organizing data before insertion

### Vector Formatting

```python
# Convert Python list to pgvector format
embedding_list = [0.1, 0.2, ..., 0.128]  # 128 dimensions
embedding_str = str(embedding_list)       # pgvector accepts string representation
```

## Common Commands

```bash
# Setup
python -m venv venv
source venv/bin/activate  # macOS/Linux
pip install -r requirements.txt

# Database operations
python db_build.py    # Create tables
python db_insert.py   # Populate data
python db_query.py    # Run queries

# Cleanup
python db_drop.py     # Drop all tables
```

## Output Format

- All deliverables in **Markdown** format
- Query results should be formatted as markdown tables
- Create results/ directory for output files
- User will handle PDF conversion later if needed

## Environment Notes

- Keep everything local and simple
- Use python -m venv (NOT conda)
- Store connection string in .env file
- Add .env and data/ to .gitignore
- Virtual environment folder (venv/) should not be tracked

## Current Status

- [x] Basic project structure created
- [x] Starter code with TODOs provided
- [ ] Implement db_build.py CREATE TABLE statements
- [ ] Implement db_insert.py data loading and insertion
- [ ] Implement db_query.py for all 6 questions
- [ ] Create results directory and output files
- [ ] Generate query results in Markdown format

## Resources

- pgvector docs: https://github.com/pgvector/pgvector
- psycopg2 tutorial: https://www.geeksforgeeks.org/executing-sql-query-with-psycopg2-in-python/
- HuggingFace dataset: https://huggingface.co/datasets/Whispering-GPT/lex-fridman-podcast
- OpenAI embeddings: https://platform.openai.com/docs/guides/embeddings

## Important Reminders

1. Always use L2 distance (`<->`) for queries
2. Exclude query document from results
3. Use fast_pg_insert for bulk operations
4. Output everything in Markdown, not PDF
5. Keep things simple and local (venv, not conda)
6. Segment IDs are formatted as "podcast_idx;segment_idx"
7. custom_id in embeddings.jsonl matches segment.id
